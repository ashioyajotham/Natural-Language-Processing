# Natural-Language-Processing
Welcome to my Natural Language Processing (NLP) diary ^_^. 

## What are transformers?
* `Transformers` are a type of neural network architecture that allow for parallelization across the sequence. This means that the network can process all of the tokens in the sequence at the same time, rather than having to process them sequentially. This is a huge advantage over RNNs, which must process tokens sequentially.
* It was introduced through the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) in 2017 which can be found in the Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017). by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin
* Below is a diagram of the Transformer architecture:
![Transformer Architecture](
https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png
)
* Sebastian Ratchka sums it well [here](https://www.linkedin.com/posts/sebastianraschka_ai-llm-transformers-activity-7074387165543092224-tlX-?utm_source=share&utm_medium=member_desktop)


****************
Fave paper so far:
***************
## [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2306.02707)
* A look at Orca and other open source LLMs.
* _Can large language models (LLMs) train themselves?_
Credits: [Cameron Wolfe](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) found through this [twitter thread](https://twitter.com/cwolferesearch/status/1673398297304911872)


### Research focus
1) Alignment
- [A closer look at RLHF](https://twitter.com/cwolferesearch/status/1724486576992886985)
2) AI Safety (particularly interested in red-teaming)
3) "Hallucination problem"
  - [My latest thought](https://www.lesswrong.com/posts/FqYyJ9Gxf6GkQQ9SE/examining-the-boundary-between-imagination-and-hallucination)
4) Finetuning v RAG


