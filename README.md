# Natural-Language-Processing
Welcome to my Natural Language Processing (NLP) diary ^_^. 

## What are transformers?
* `Transformers` are a type of neural network architecture that allow for parallelization across the sequence. This means that the network can process all of the tokens in the sequence at the same time, rather than having to process them sequentially. This is a huge advantage over RNNs, which must process tokens sequentially.
* It was introduced through the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) in 2017 by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin
* Below is a diagram of the Transformer architecture:
![Transformer Architecture](
  https://miro.medium.com/max/1400/1*9gBC9o9X-pQZ-2-6k6xViw.png
)
* The Transformer architecture has been used in a variety of tasks, including machine
translation, text summarization, and image captioning.
* Sebastian Ratchka sums it well [here](https://www.linkedin.com/posts/sebastianraschka_ai-llm-transformers-activity-7074387165543092224-tlX-?utm_source=share&utm_medium=member_desktop)


## RASA (Open Source Conversational AI)
* `RASA` is an open source machine learning framework to automate text-and voice-based conversations. With Rasa, you can build chatbots on:
  * Facebook Messenger
  * Slack
  * Microsoft Bot Framework
  * Rocket.Chat
  * Mattermost
  * Telegram
  * Twilio
  * Your own custom conversational channels with the open source SDK combining other chatbots like ChatGPT
* It was introduced through the paper [Rasa: Open source language understanding and dialogue management](https://arxiv.org/abs/1712.05181) in 2017 by Tom Bocklisch, Philipp Blandfort, Tobias Brox, Nick Pawlowski, Alan Nichol, Mete Sertkan, Johannes Mosig and Alan Telešič



****************
Fave paper so far:
***************
## [Imitation Learning in LLMs](https://arxiv.org/abs/2306.02707)
* A look at Orca and other open source LLMs.
* _Can large language models (LLMs) train themselves?_
Credits: [Cameron Wolfe](https://cameronrwolfe.substack.com/p/orca-properly-imitating-proprietary) found through this [twitter thread](https://twitter.com/cwolferesearch/status/1673398297304911872)



### Future Work
1) Transformers & LLMs 
- This will focus on what 2017 NLP researchers have called "Transformer" models, which are a class of models that leverage the attention mechanism.
- These models are based on the Transformer architecture, which uses a stack of self-attention layers to draw global dependencies between input and output.
- The Transformer architecture is the basis for many of the most recent NLP models, including BERT, GPT-2, XLNet, and T5.
