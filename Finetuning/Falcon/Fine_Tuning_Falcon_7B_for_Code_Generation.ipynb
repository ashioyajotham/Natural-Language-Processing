{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmHkiAq80mtNDdy/UDjkx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5569868089cf4f92aabb895a1401a4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7975616f9014530a8d2523285f806e0",
              "IPY_MODEL_cfb01f3e70854e83b09e8ecace1e009a",
              "IPY_MODEL_40797f199a3e4f82b07c6b766085ab87"
            ],
            "layout": "IPY_MODEL_720d4e1a65ab41fa99975d5f392e6d36"
          }
        },
        "d7975616f9014530a8d2523285f806e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb35c7aef5b64ec4ade7de2e221a3cb2",
            "placeholder": "​",
            "style": "IPY_MODEL_71e171d4ac9647ee9402e1acbf92e1a6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cfb01f3e70854e83b09e8ecace1e009a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f74b529a04f64a498f97550a99b5d7ef",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aeaf55df1ad149dc9f92da645133eda8",
            "value": 8
          }
        },
        "40797f199a3e4f82b07c6b766085ab87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a7135624c9461483b6415b084d1cd8",
            "placeholder": "​",
            "style": "IPY_MODEL_9be8b078663e4bd298dc0310ec498a50",
            "value": " 8/8 [01:30&lt;00:00,  9.15s/it]"
          }
        },
        "720d4e1a65ab41fa99975d5f392e6d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb35c7aef5b64ec4ade7de2e221a3cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e171d4ac9647ee9402e1acbf92e1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f74b529a04f64a498f97550a99b5d7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeaf55df1ad149dc9f92da645133eda8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1a7135624c9461483b6415b084d1cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be8b078663e4bd298dc0310ec498a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashioyajotham/Natural-Language-Processing/blob/main/Finetuning/Falcon/Fine_Tuning_Falcon_7B_for_Code_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85DqD0_nxXxa",
        "outputId": "38fb5149-0d8b-4524-86b7-495549284004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Specify the name of the dataset\n",
        "#dataset_name = \"yahma/alpaca-cleaned\"\n",
        "dataset_name = \"tatsu-lab/alpaca\"\n",
        "\n",
        "\n",
        "# Load the dataset from the specified name and select the \"train\" split\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "XfBwpMcDxt6k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be loading the Falcon 7B model, applying 4bit quantization to it, and then adding LoRA adapters to the model.\n",
        "import torch\n",
        "\n",
        "from transformers import FalconForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Defining the name of the Falcon model\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "# Configuring the BitsAndBytes quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "device_map = 'auto',\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Loading the Falcon model with quantization configuration\n",
        "model = FalconForCausalLM.from_pretrained(\n",
        "model_name,\n",
        "quantization_config=bnb_config,\n",
        "trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Disabling cache usage in the model configuration\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "5569868089cf4f92aabb895a1401a4c9",
            "d7975616f9014530a8d2523285f806e0",
            "cfb01f3e70854e83b09e8ecace1e009a",
            "40797f199a3e4f82b07c6b766085ab87",
            "720d4e1a65ab41fa99975d5f392e6d36",
            "eb35c7aef5b64ec4ade7de2e221a3cb2",
            "71e171d4ac9647ee9402e1acbf92e1a6",
            "f74b529a04f64a498f97550a99b5d7ef",
            "aeaf55df1ad149dc9f92da645133eda8",
            "f1a7135624c9461483b6415b084d1cd8",
            "9be8b078663e4bd298dc0310ec498a50"
          ]
        },
        "id": "Quz3iEltzIHZ",
        "outputId": "7033f8dc-16a4-4b8a-8ecd-860a6bac3bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "You are using a model of type RefinedWebModel to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5569868089cf4f92aabb895a1401a4c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for the Falcon 7B model with remote code trust\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "QDYnoc7lzqjJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module for LoRA configuration\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Define the parameters for LoRA configuration\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "# Create the LoRA configuration object\n",
        "peft_config = LoraConfig(\n",
        "lora_alpha=lora_alpha,\n",
        "lora_dropout=lora_dropout,\n",
        "r=lora_r,\n",
        "bias=\"none\",\n",
        "task_type=\"CAUSAL_LM\",\n",
        "target_modules=[\n",
        "\"query_key_value\",\n",
        "\"dense\",\n",
        "\"dense_h_to_4h\",\n",
        "\"dense_4h_to_h\",\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "iw35SeUT0UhP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "# Define the directory to save training results\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Set the batch size per device during training\n",
        "per_device_train_batch_size = 1\n",
        "\n",
        "# Number of steps to accumulate gradients before updating the model\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Choose the optimizer type (e.g., \"paged_adamw_32bit\")\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Interval to save model checkpoints (every 10 steps)\n",
        "save_steps = 10\n",
        "\n",
        "# Interval to log training metrics (every 10 steps)\n",
        "logging_steps = 10\n",
        "\n",
        "# Learning rate for optimization\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Maximum gradient norm for gradient clipping\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Maximum number of training steps\n",
        "max_steps = 20\n",
        "\n",
        "# Warmup ratio for learning rate scheduling\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Type of learning rate scheduler (e.g., \"constant\")\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Create a TrainingArguments object to configure the training process\n",
        "training_arguments = TrainingArguments(\n",
        "output_dir=output_dir,\n",
        "per_device_train_batch_size=per_device_train_batch_size,\n",
        "gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "optim=optim,\n",
        "save_steps=save_steps,\n",
        "logging_steps=logging_steps,\n",
        "learning_rate=learning_rate,\n",
        "fp16=True,  # Use mixed precision training (16-bit)\n",
        "max_grad_norm=max_grad_norm,\n",
        "max_steps=max_steps,\n",
        "warmup_ratio=warmup_ratio,\n",
        "group_by_length=True,\n",
        "lr_scheduler_type=lr_scheduler_type,\n",
        ")"
      ],
      "metadata": {
        "id": "vtfNVrv40ZSA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda x: {\"text\": x[\"input\"]+x[\"output\"]})\n",
        "\n",
        "# Import the SFTTrainer from the TRL library\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Set the maximum sequence length\n",
        "max_seq_length = 512\n",
        "\n",
        "# Create a trainer instance using SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "model=model,\n",
        "train_dataset=dataset,\n",
        "peft_config=peft_config,\n",
        "dataset_text_field=\"text\",\n",
        "max_seq_length=max_seq_length,\n",
        "tokenizer=tokenizer,\n",
        "args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "DJhbDreS0bJf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the named modules of the trainer's model\n",
        "for name, module in trainer.model.named_modules():\n",
        "\n",
        "# Check if the name contains \"norm\"\n",
        "  if \"norm\" in name:\n",
        "\t  # Convert the module to use torch.float32 data type\n",
        "\t  module = module.to(torch.float32)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "XK6bNBMI6-5M",
        "outputId": "54f0da26-8093-4833-dbae-1ecdc4c22af6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashioyajotham\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231024_042731-pd7kqzut</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ashioyajotham/huggingface/runs/pd7kqzut' target=\"_blank\">tough-forest-15</a></strong> to <a href='https://wandb.ai/ashioyajotham/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ashioyajotham/huggingface' target=\"_blank\">https://wandb.ai/ashioyajotham/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ashioyajotham/huggingface/runs/pd7kqzut' target=\"_blank\">https://wandb.ai/ashioyajotham/huggingface/runs/pd7kqzut</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 01:54, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.691700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.644300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20, training_loss=1.6680020332336425, metrics={'train_runtime': 126.1853, 'train_samples_per_second': 0.634, 'train_steps_per_second': 0.158, 'total_flos': 367056368432640.0, 'train_loss': 1.6680020332336425, 'epoch': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./falcon-7b-code-gen\")\n",
        "#tokenizer.save_pretrained(\"./tokenizer\")"
      ],
      "metadata": {
        "id": "GQWPlpB9vac2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Dw4Hq5oP6eTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "\n",
        "whoami()\n",
        "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
      ],
      "metadata": {
        "id": "iOI0NSd08bdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(repo_id=\"super-cool-model\")\n"
      ],
      "metadata": {
        "id": "_GSVG3IC53Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"super-cool-model\")"
      ],
      "metadata": {
        "id": "iLZ6AtiC-etG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "sequences = pipeline(\n",
        "   \"Generate a python script to create random numbers between 10 and 100\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGqy6kb18vcu",
        "outputId": "49bc4cd0-9aef-4a7c-ec7e-7286214b65f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: Generate a python script to create random numbers between 10 and 100\n",
            "This tutorial will teach you how to generate a random number between two numbers. It will generate a random number between 10 and 100 based on the user input. It will also show you how to create a python script to create random numbers between 10 and 100.\n",
            "To generate a python script to create random numbers, you must first define two variables. The first variable will store the range of numbers between which the random numbers will be generated. The second variable will store the number of numbers that will be generated. You must first assign a random number between 10 and 100 to the range variable. Then use the range variable to create a random number between 10 and 100 and assign it to the second variable. Finally, create a python script to create random numbers between 10 and 100.\n",
            "This script will take the two variables as input and use the values to generate random numbers between 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MPZqN7j_PIh",
        "outputId": "b450bbb7-0729-459f-cac4-c05554292fcd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "What is your favourite condiment?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!<|im_end|>\n",
            "<|im_start|>user\n",
            "Do you have mayonnaise recipes?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Absolutely! Want me to share some? I can make you an amazing egg salad, or a tasty potato salad. I can even whip up some homemade mayo to go with your burgers!<|im_end|>\n",
            "\n",
            "</div>/end snippet>\n",
            "\n",
            "<script src=\"script.js\"></script>\n",
            "\n",
            "</body>\n",
            "</html>\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model = model, tokenizer=tokenizer)\n",
        "generator(\"The Bible is\", max_length = 100, do_sample = True, num_return_sequences=1)\n",
        "## [{'generated_text': \"Hello, I'm a language modeler. So while writing this, when I went out to meet my wife or come home she told me that my\"},\n",
        "##  {'generated_text': \"Hello, I'm a language modeler. I write and maintain software in Python. I love to code, and that includes coding things that require writing\"}, ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWdfvD5QA5Or",
        "outputId": "9ab5153f-ce36-4ccd-c1f7-f15d275ff08c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The Bible is still a long way from it, and its most valuable and precious texts are now being replaced by a new generation of believers, and the best they can do is to continue building the Bible to help you with your lives and to learn from God Himself and His message in a new way.\\n\\nThe Best in the World was founded in 2003 and is designed to help you grow, strengthen, inspire, and expand life a bit. To celebrate the beginning of our new generation of believers:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}