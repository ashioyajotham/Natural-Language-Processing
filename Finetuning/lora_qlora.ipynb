{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem](https://pytorch.org/blog/finetune-llms/?utm_content=278057354&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMETER EFFICIENT FINE-TUNING (PEFT) METHODS\n",
    "* PEFT methods aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) LOW-RANK ADAPTATION FOR LARGE LANGUAGE MODELS (LORA) USING ðŸ¤— PEFT\n",
    "* The LoRA method by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model\n",
    "* To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices (called update matrices). These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesnâ€™t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n",
    "* This approach has several advantages:\n",
    "\n",
    "    - LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.\n",
    "    - The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA    models for various downstream tasks built on top of them.\n",
    "    - LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.     \n",
    "    - The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.\n",
    "    - LoRA does not add any inference latency when adapter weights are merged with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Create peft config\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules= [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Create PeftModel which inserts LoRA adapters using the above config\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "# Train the model using HF Trainer/ HF accelerate/ custom loop\n",
    "\n",
    "# Save the adapter weights\n",
    "model.save_adapter(\"my_awesome_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) QLORA: ONE OF THE CORE CONTRIBUTIONS OF BITSANDBYTES TOWARDS THE DEMOCRATIZATION OF AI\n",
    "* According to the LoRA formulation, the base model can be compressed in any data type (â€˜dtypeâ€™) as long as the hidden states from the base model are in the same dtype as the output hidden states from the LoRA matrices.\n",
    "* Compressing and quantizing large language models has recently become an exciting topic as SOTA models become larger and more difficult to serve and use for end users. Many people in the community proposed various approaches for effectively compressing LLMs with minimal performance degradation.\n",
    "* This is where the `bitsandbytes` library comes in. Quantization of LLMs has largely focused on quantization for inference, but the `QLoRA (Quantized model weights + Low-Rank Adapters)` paper showed the breakthrough utility of using backpropagation through frozen, quantized weights at large model scales.\n",
    "\n",
    "*  To use LLM.int8 and QLoRA algorithms, respectively, simply pass `load_in_8bit` and `load_in_4bit` to the from_pretrained method.\n",
    "\n",
    "* In addition to generous use of LoRA, to achieve high-fidelity fine-tuning of 4-bit models, QLoRA uses 3 further algorithmic tricks:\n",
    "\n",
    "    - 4-bit NormalFloat (NF4) quantization, a custom data type exploiting the property of the normal distribution of model - weights and distributing an equal number of weights (per block) to each quantization binâ€”thereby enhancing information density.\n",
    "    - Double Quantization, quantization of the quantization constants (further savings).\n",
    "    - Paged Optimizers, preventing memory spikes during gradient checkpointing from causing out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "\n",
    "# For LLM.int8()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "\n",
    "# For QLoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training QLoRA Model using HuggingFace PEFT Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Autotokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, Tasktype, get_peft_model\n",
    "\n",
    "# Create quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    ")\n",
    "\n",
    "# Base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Prepare model for quantized training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Create peft config\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules= [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Create PeftModel which inserts LoRA adapters using the above config\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Train the model using HF Trainer/ HF accelerate/ custom loop\n",
    "\n",
    "# Save the adapter weights\n",
    "model.save_adapter(\"my_awesome_adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
