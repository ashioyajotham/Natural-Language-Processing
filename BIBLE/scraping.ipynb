{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.bible.com/bible/286/PSA.1.ZUL59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter = soup.findAll('div', class_ = 'chapter ch1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter[0].findAll('span',{'class':['label',  'content']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chapter[0].findAll('span',class_ = 'verse v1')[1].findAll('span', class_ = 'content')[0].text)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = chapter[0].findAll('span', class_ = 'label') #+str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0].findAll('span', class_ = 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing each chapter to csv, for each chapter\n",
    "#write below code as a function that takes in chapter in a list of length 1\n",
    "#this chapter is return from getNextChapter function\n",
    "# getNextChapter takes current page body by sending request,\n",
    "# and gets url of next chapter, then returns the chapter in a list of length 1\n",
    "\n",
    "\n",
    "# \n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_btn = soup.find('a', class_ = 'bible-nav-button nav-right fixed dim br-100 ba b--black-20 pa2 pa3-m flex items-center justify-center bg-white right-1').get('href')\n",
    "# bible-nav-button nav-right fixed dim br-100 ba b--black-20 pa2 pa3-m flex items-center justify-center bg-white right-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_btn     #https://www.bible.com/en-GB/bible/111/PSA.2.NIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = 'https://www.bible.com/'+ next_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chapter is return from getNextChapter function\n",
    "# getNextChapter takes current page body response by sending request,\n",
    "# and gets url of next chapter, then returns the soup\n",
    "\n",
    "def getNextChapter(res):\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    next_chpt = soup.find('a', class_ = 'bible-nav-button nav-right fixed dim br-100 ba b--black-20 pa2 pa3-m flex items-center justify-center bg-white right-1').get('href')\n",
    "    link = 'https://www.bible.com/'+ next_chpt\n",
    "#     response = requests.get(link)\n",
    "#     bsp = BeautifulSoup(response.text, 'html.parser')\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing each chapter to csv, for each chapter\n",
    "#write below code as a function that takes in chapter in a list of length 1\n",
    "\n",
    "def scrape_to_csv(soup, chptr):\n",
    "    chapter = soup.findAll('div', class_ = 'chapter ch'+str(chptr))\n",
    "    scriptures = []\n",
    "    num_verses = len(chapter[0].findAll('span', class_ = 'label'))\n",
    "    with open('amahubo.csv', 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(0,num_verses):\n",
    "            verse = []\n",
    "            for x in chapter[0].findAll('span', class_ = 'verse v'+str(i+1)):\n",
    "                if len(x.findAll('span', class_ = 'content'))==1:\n",
    "                    x = x.findAll('span', class_ = 'content')[0]\n",
    "                else:\n",
    "                    x = x.findAll('span', class_ = 'content')[1]\n",
    "                verse.append(re.sub(r'\\d+', '', x.text)) # append parts of verse\n",
    "            v_final = ' '.join(verse)\n",
    "            scriptures.append(v_final)\n",
    "            writer.writerow(['Verse '+str(i+1), scriptures[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def next_url(res):\n",
    "#     resp_url = getNextChapter(response) #link to chapter 2\n",
    "#     resp = requests.get(resp_url) #200 for chapter 2\n",
    "#     bsp = BeautifulSoup(resp.text, 'html.parser') #soup for chapter 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping chapter  2\n",
      "scraping chapter  3\n",
      "scraping chapter  4\n",
      "scraping chapter  5\n",
      "scraping chapter  6\n",
      "scraping chapter  7\n",
      "scraping chapter  8\n",
      "scraping chapter  9\n",
      "scraping chapter  10\n",
      "scraping chapter  11\n",
      "scraping chapter  12\n",
      "scraping chapter  13\n",
      "scraping chapter  14\n",
      "scraping chapter  15\n",
      "scraping chapter  16\n",
      "scraping chapter  17\n",
      "scraping chapter  18\n",
      "scraping chapter  19\n",
      "scraping chapter  20\n",
      "scraping chapter  21\n",
      "scraping chapter  22\n",
      "scraping chapter  23\n",
      "scraping chapter  24\n",
      "scraping chapter  25\n",
      "scraping chapter  26\n",
      "scraping chapter  27\n",
      "scraping chapter  28\n",
      "scraping chapter  29\n",
      "scraping chapter  30\n",
      "scraping chapter  31\n",
      "scraping chapter  32\n",
      "scraping chapter  33\n",
      "scraping chapter  34\n",
      "scraping chapter  35\n",
      "scraping chapter  36\n",
      "scraping chapter  37\n",
      "scraping chapter  38\n",
      "scraping chapter  39\n",
      "scraping chapter  40\n",
      "scraping chapter  41\n",
      "scraping chapter  42\n",
      "scraping chapter  43\n",
      "scraping chapter  44\n",
      "scraping chapter  45\n",
      "scraping chapter  46\n",
      "scraping chapter  47\n",
      "scraping chapter  48\n",
      "scraping chapter  49\n",
      "scraping chapter  50\n",
      "scraping chapter  51\n",
      "scraping chapter  52\n",
      "scraping chapter  53\n",
      "scraping chapter  54\n",
      "scraping chapter  55\n",
      "scraping chapter  56\n",
      "scraping chapter  57\n",
      "scraping chapter  58\n",
      "scraping chapter  59\n",
      "scraping chapter  60\n",
      "scraping chapter  61\n",
      "scraping chapter  62\n",
      "scraping chapter  63\n",
      "scraping chapter  64\n",
      "scraping chapter  65\n",
      "scraping chapter  66\n",
      "scraping chapter  67\n",
      "scraping chapter  68\n",
      "scraping chapter  69\n",
      "scraping chapter  70\n",
      "scraping chapter  71\n",
      "scraping chapter  72\n",
      "scraping chapter  73\n",
      "scraping chapter  74\n",
      "scraping chapter  75\n",
      "scraping chapter  76\n",
      "scraping chapter  77\n",
      "scraping chapter  78\n",
      "scraping chapter  79\n",
      "scraping chapter  80\n",
      "scraping chapter  81\n",
      "scraping chapter  82\n",
      "scraping chapter  83\n",
      "scraping chapter  84\n",
      "scraping chapter  85\n",
      "scraping chapter  86\n",
      "scraping chapter  87\n",
      "scraping chapter  88\n",
      "scraping chapter  89\n",
      "scraping chapter  90\n",
      "scraping chapter  91\n",
      "scraping chapter  92\n",
      "scraping chapter  93\n",
      "scraping chapter  94\n",
      "scraping chapter  95\n",
      "scraping chapter  96\n",
      "scraping chapter  97\n",
      "scraping chapter  98\n",
      "scraping chapter  99\n",
      "scraping chapter  100\n",
      "scraping chapter  101\n",
      "scraping chapter  102\n",
      "scraping chapter  103\n",
      "scraping chapter  104\n",
      "scraping chapter  105\n",
      "scraping chapter  106\n",
      "scraping chapter  107\n",
      "scraping chapter  108\n",
      "scraping chapter  109\n",
      "scraping chapter  110\n",
      "scraping chapter  111\n",
      "scraping chapter  112\n",
      "scraping chapter  113\n",
      "scraping chapter  114\n",
      "scraping chapter  115\n",
      "scraping chapter  116\n",
      "scraping chapter  117\n",
      "scraping chapter  118\n",
      "scraping chapter  119\n",
      "scraping chapter  120\n",
      "scraping chapter  121\n",
      "scraping chapter  122\n",
      "scraping chapter  123\n",
      "scraping chapter  124\n",
      "scraping chapter  125\n",
      "scraping chapter  126\n",
      "scraping chapter  127\n",
      "scraping chapter  128\n",
      "scraping chapter  129\n",
      "scraping chapter  130\n",
      "scraping chapter  131\n",
      "scraping chapter  132\n",
      "scraping chapter  133\n",
      "scraping chapter  134\n",
      "scraping chapter  135\n",
      "scraping chapter  136\n",
      "scraping chapter  137\n",
      "scraping chapter  138\n",
      "scraping chapter  139\n",
      "scraping chapter  140\n",
      "scraping chapter  141\n",
      "scraping chapter  142\n",
      "scraping chapter  143\n",
      "scraping chapter  144\n",
      "scraping chapter  145\n",
      "scraping chapter  146\n",
      "scraping chapter  147\n",
      "scraping chapter  148\n",
      "scraping chapter  149\n",
      "scraping chapter  150\n",
      "Done with all 150 chapters\n"
     ]
    }
   ],
   "source": [
    "# request page, get body and chapter content, scrape chapter 1 and write to csv\n",
    "# call getNextChapter for chpt 2\n",
    "# now we have chapter 2 in a list\n",
    "# we set chpt = 2, then in a while loop :\n",
    "\n",
    "# while chpt <=150 then call scraper/writer function\n",
    "# this writes chpt content to csv\n",
    "# then call get next chapter function\n",
    "# which returns content of chtp 3\n",
    "# now we have chapter 3 in a list\n",
    "# then we increment chpt ++\n",
    "url = 'https://www.bible.com/bible/286/PSA.1.ZUL59'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "chapter = soup.findAll('div', class_ = 'chapter ch1')\n",
    "# chapter[0].findAll('span', class_ = 'verse v1') #+str(1) ???\n",
    "\n",
    "with open('amahubo.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['VERSE', 'SCRIPTURE'])\n",
    "    \n",
    "scriptures = []\n",
    "verses_num = len(chapter[0].findAll('span',{'class':['label',  'content']}))\n",
    "with open('amahubo.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(0,verses_num):\n",
    "        verse = []\n",
    "        for x in chapter[0].findAll('span', class_ = 'verse v'+str(i+1)):\n",
    "            if len(x.findAll('span', class_ = 'content'))==1:\n",
    "                x = x.findAll('span', class_ = 'content')[0]\n",
    "            else:\n",
    "                x = x.findAll('span', class_ = 'content')[1]\n",
    "            verse.append(re.sub(r'\\d+', '', x.text)) # append parts of verse\n",
    "        v_final = ' '.join(verse)\n",
    "        scriptures.append(v_final)\n",
    "        writer.writerow(['Verse '+str(i+1), scriptures[i]])\n",
    "\n",
    "resp_url = getNextChapter(response) #link to chapter 2\n",
    "resp = requests.get(resp_url) #200 for chapter 2\n",
    "bsp = BeautifulSoup(resp.text, 'html.parser') #soup for chapter 2\n",
    "\n",
    "chpt = 2\n",
    "while(chpt<=150):\n",
    "    print('scraping chapter ',chpt)\n",
    "    scrape_to_csv(bsp,chpt) # this writes current chapter content to csv , current = 2\n",
    "    resp_url = getNextChapter(resp) #gets link to next chapter using curr link\n",
    "    resp = requests.get(resp_url)\n",
    "    bsp = BeautifulSoup(resp.text, 'html.parser')\n",
    "#     print('finished scaping chapter ',chpt)\n",
    "    chpt +=1\n",
    "    \n",
    "    \n",
    "print('Done with all 150 chapters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load first page\n",
    "#### Scrape chapter\n",
    "#### Get next chapter url\n",
    "#### Load next page\n",
    "#### Scrape charpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
